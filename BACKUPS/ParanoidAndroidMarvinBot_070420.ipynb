{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ParanoidAndroidMarvinBot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNQ01z1id2jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install telebot\n",
        "#!pip3 uninstall telebot\n",
        "!pip3 install pytelegrambotapi --upgrade > /dev/null \n",
        "!pip install pymorphy2 > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nzoKPE_vnco",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "4e826259-ad5a-4b6c-f3ef-207a15260769"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk \n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.stem import wordnet # to perform lemmitization\n",
        "from sklearn.feature_extraction.text import CountVectorizer # to perform bow\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # to perform tfidf\n",
        "from nltk import pos_tag # for parts of speech\n",
        "from sklearn.metrics import pairwise_distances # to perfrom cosine similarity\n",
        "from nltk import word_tokenize # to create tokens\n",
        "from nltk.corpus import stopwords # for stop words\n",
        "\n",
        "import telebot\n",
        "import random\n",
        "import time\n",
        "\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import pymorphy2\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "tfidf = TfidfVectorizer()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE9u75QF3RI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stickers_ids = ['CAACAgIAAxkBAAN2XohnGn0E2PpnDa8sLRjIfoXJkMgAAucCAAJjK-IJgZ3pFN1v78MYBA',\n",
        "'CAACAgIAAxkBAAN6XohnHcqqlO0fWzeP5hyVEW-Q0tUAAswCAAJjK-IJzL8QiJ5hkasYBA',\n",
        "'CAACAgIAAxkBAAODXohnJLq7B2-qL_YDJwpvUNp60EAAAqgCAAJjK-IJ0Knq1taB2AEYBA',\n",
        "'CAACAgIAAxkBAAOHXohnJjVasUvm0at_qVDuO7AN_s8AAqICAAJjK-IJDEdBkfGfrdYYBA',\n",
        "'CAACAgIAAxkBAAOJXohnJ9H6Ra8cxM5szXoW1j0AAa5_AAKcAgACYyviCVvc75FsiA97GAQ',\n",
        "'CAACAgIAAxkBAAN_XohnIa7dO6ISHOtT8iIWr69aZJ8AAroCAAJjK-IJbneXMn3Iye8YBA', \n",
        "'CAACAgIAAxkBAAN-XohnIN7xjPDIDGCH5LWx4LkiGdUAArwCAAJjK-IJ-4zaoKwb82cYBA', \n",
        "'CAACAgIAAxkBAAN0XohnGEd0ZCsu3v7ZVow44enNBUAAAtoCAAJjK-IJauzw6tbgTlgYBA', \n",
        "'CAACAgIAAxkBAAN1XohnGfEueVTYaiDyaJXZzknDzyYAAtgCAAJjK-IJ0c93YMe3wtsYBA', \n",
        "'CAACAgIAAxkBAAN3XohnGsaRKK2tQHUikHhW3MVNxKsAAtYCAAJjK-IJscEHEE2deiEYBA', \n",
        "'CAACAgIAAxkBAAN4XohnG3HoKJhy_rjeND70rx9-29wAAsYCAAJjK-IJzec08mzFJQ4YBA', \n",
        "'CAACAgIAAxkBAAN5XohnHfV86f6rIs8PFAOuTzu4hk4AAsoCAAJjK-IJQ-opywa_2tMYBA', \n",
        "'CAACAgIAAxkBAAN7XohnHphjjO53yx1rXeW7dfX0BQ0AAtICAAJjK-IJUJFHXqol5P0YBA', \n",
        "'CAACAgIAAxkBAAN9XohnIOVsBaftETwrCTzgMY3P-uQAAsQCAAJjK-IJfFM_LRL30PMYBA', \n",
        "'CAACAgIAAxkBAANoXohnEGpDZr7m28TSltOJfdliPssAAhQDAAJjK-IJaxaD98tkXc8YBA', \n",
        "'CAACAgIAAxkBAANpXohnEkM5ZS-eeuVIjVpEs9SFm-kAAhIDAAJjK-IJkw6POAaUJz0YBA', \n",
        "'CAACAgIAAxkBAANqXohnE--5DGSMCm1Xvqqfmi4VjysAAg4DAAJjK-IJF3ZWXqnR2U4YBA', \n",
        "'CAACAgIAAxkBAANrXohnE1BGcHiL9ChQ9IhwOPi0zbYAAgwDAAJjK-IJVBdFLu9Z334YBA', \n",
        "'CAACAgIAAxkBAANsXohnFHhivGP6HdfRQ8KbROuWZmEAAggDAAJjK-IJ3pWODHATQt0YBA', \n",
        "'CAACAgIAAxkBAANuXohnFYx7Wu3QYUM1kNVfUrXQW3IAAt4CAAJjK-IJkEJR_BL_n5MYBA', \n",
        "'CAACAgIAAxkBAANvXohnFpGeKuTvxJfV8l3oOVlPvO0AAuMCAAJjK-IJxoTo3R-Dn4kYBA', \n",
        "'CAACAgIAAxkBAANwXohnFqrdM5JEVWsBkXEofbeQ-jEAAuwCAAJjK-IJ1ybfVvDJmr8YBA', \n",
        "'CAACAgIAAxkBAANyXohnFxrgfKFYdPhFBxqzzwO2bjYAAgIDAAJjK-IJM1tbM7mpBCsYBA', \n",
        "'CAACAgIAAxkBAANzXohnGJJaNnPDbxvZh56kQX8fr9IAAtwCAAJjK-IJS8TCPIGDi8kYBA', \n",
        "'CAACAgIAAxkBAAOAXohnInsqNljONwUy9Wf2IhngslAAArgCAAJjK-IJO9-36Nk_9JsYBA', \n",
        "'CAACAgIAAxkBAAOBXohnIlD94WdP7bz2sZ1FNQcOgloAArICAAJjK-IJXtqn3ejnpP0YBA', \n",
        "'CAACAgIAAxkBAAOCXohnI8cuZa9UhsOJjL770GdURYgAAqQCAAJjK-IJekxACxNzwcQYBA', \n",
        "'CAACAgIAAxkBAAOEXohnJPK1QPhDT1zTW_IV1LN-mrgAAqoCAAJjK-IJdjaSdZSUJbYYBA', \n",
        "'CAACAgIAAxkBAAOFXohnJVj2igpN_C_wLS8r5pRYUPoAAqwCAAJjK-IJeBtXRF_LpwsYBA', \n",
        "'CAACAgIAAxkBAAOGXohnJczgrPUt_QFTDBqym7zHRbsAAq4CAAJjK-IJJUGyNW_KPTEYBA', \n",
        "'CAACAgIAAxkBAAOIXohnJ8Wr-DyuC6tvStjtZJ2WYRkAAqACAAJjK-IJ5eT-NU0XyOAYBA', \n",
        "'CAACAgIAAxkBAAOKXohnKM0206ZKxdgmOb8KaAbZ4QsAApoCAAJjK-IJI1GLrSIwyv0YBA', \n",
        "'CAACAgIAAxkBAAOLXohnKNeIj-VbxTLW8g8zWBY3Hv4AApYCAAJjK-IJTfSNA206rU4YBA', \n",
        "'CAACAgIAAxkBAAOMXohnKmfPVWS3KGEVU_3H59Y959oAApgCAAJjK-IJZadb5UJwT84YBA', \n",
        "'CAACAgIAAxkBAAOOXohnK6HTXiY2gzOsvRBIjYKKJVoAAhYDAAJjK-IJ_jzqyKdDpjsYBA', \n",
        "'CAACAgIAAxkBAAOPXohnLMMSLd0plo22Id73oxkdT5AAAp4CAAJjK-IJxptc4DRhmEQYBA', \n",
        "'CAACAgIAAxkBAAOQXohnLa5XUAsjiPePFeRC8oeAqDEAAh0GAAJjK-IJOM2IMvQWyocYBA', \n",
        "'CAACAgIAAxkBAAN8XohnHnnwHrkKy9KMMnfDf_vyQqMAAtQCAAJjK-IJEaAVfvqwVXoYBA', \n",
        "'CAACAgIAAxkBAANtXohnFaYStFiZQOlvY_p_vap0WkMAAgQDAAJjK-IJ5Bhot_PLURcYBA', \n",
        "'CAACAgIAAxkBAAONXohnKgbBchQUV-4XGEwus7cL1-sAAuECAAJjK-IJ-1wpHc7C8HgYBA', \n",
        "'CAACAgIAAxkBAANxXohnF1wbHy_l4IlMWx4qGThEkYcAAwMAAmMr4gk_-9kTKMo6YBgE']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlYkMvoN7N2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questions = ['Какой фильм тебе нравится?' , 'Дай мне название какого-нибудь фильма.']\n",
        "answers = ['Ты продолжай, когда я тебя пойму, я тебе отвечу...' , 'Давай, накидывай, я разберусь...' , 'Тебя родные хоть понимают?']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9xVGpCzeKTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_stop_words = stopwords.words('russian')\n",
        "my_stop_words.extend(['что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', '–', 'к', 'на', '...'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmqlCt_qZLYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_ru(file_text):\n",
        "# firstly let's apply nltk tokenization\n",
        "  tokens = word_tokenize(file_text)\n",
        "\n",
        "# let's delete punctuation symbols\n",
        "  tokens = [i for i in tokens if (i not in string.punctuation)]\n",
        "\n",
        "# deleting stop_words\n",
        "  # stop_words = stopwords.words('russian')\n",
        "  # stop_words.extend(['что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', '–', 'к', 'на', '...'])\n",
        "  tokens = [i for i in tokens if (i not in my_stop_words)]\n",
        "\n",
        "# cleaning words\n",
        "  tokens = [i.replace(\"«\", \"\").replace(\"»\", \"\") for i in tokens]\n",
        "  tokens = [ morph.parse(i)[0].normal_form for i in tokens ]\n",
        "  return str(tokens).strip('[]').replace(\"'\",\"\").replace(\",\",\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTJTt6MQ27Zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df = pd.read_csv('movie.csv')\n",
        "\n",
        "# tfidf = TfidfVectorizer(stop_words='english')\n",
        "# tfidf_matrix = tfidf.fit_transform(df['common'])\n",
        "\n",
        "# cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "# indices = pd.Series(df.index, index=df['original_title']).drop_duplicates()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOp9FABIZv8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_top250 = pd.read_csv('top250.csv', sep='#' , names=['original_title','year', 'country' , 'score' , 'overview','director','screenplay' , 'actors' , 'url'])\n",
        "df_top250['overview_tokenize'] = df_top250['overview'].map(lambda x: tokenize_ru(str(x)))\n",
        "df_top250['original_title_lower'] = df_top250['original_title'].map(lambda x: x.lower().strip())\n",
        "df_top250['creators'] = df[df.columns[5:8]].apply(\n",
        "    lambda x: ','.join( x.dropna().astype(str)),\n",
        "    axis=1).apply(lambda x: str(x).replace(\" \",\"\").replace(\",\",\" \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCkcKUH0cFnK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a6f63dfe-9ec6-44b0-d212-f7d07f4bdb00"
      },
      "source": [
        "print(df_top250['creators'][13])"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "КвентинТарантино КвентинТарантино РоджерЭвери ДжонТраволта СэмюэлЛ.Джексон БрюсУиллис УмаТурман ВингРеймз\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvzTdOzD2JYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for test\n",
        "tfidf = TfidfVectorizer(stop_words=my_stop_words)\n",
        "tfidf_matrix = tfidf.fit_transform(df_top250['overview_tokenize'])\n",
        "\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "indices = pd.Series(df_top250.index, index=df_top250['original_title_lower']).drop_duplicates()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rERrnIuryLeB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "d8044542-0ce3-47b4-b935-73eac00841d6"
      },
      "source": [
        "\"\"\"\n",
        "tfidf_matrix_overview = tfidf.fit_transform(df_top250['overview_tokenize'])\n",
        "cosine_sim = linear_kernel(tfidf_matrix_people, tfidf_matrix_people)\n",
        "indices = pd.Series(df.index, index=df_top250['original_title_lower']).drop_duplicates()\n",
        "\"\"\""
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntfidf_matrix_overview = tfidf.fit_transform(df_top250['overview_tokenize'])\\ncosine_sim = linear_kernel(tfidf_matrix_people, tfidf_matrix_people)\\nindices = pd.Series(df.index, index=df_top250['original_title_lower']).drop_duplicates()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL-dJAfeLsfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_recommendations(title, cosine_sim=cosine_sim):\n",
        "    idx = indices[title]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:11]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    a = df_top250['original_title_lower'].iloc[movie_indices]\n",
        "    #a = str(a.values).strip('[]').replace(\"' '\" , '\\n').replace(\"'\" , \"\").strip()\n",
        "    #print(sim_scores)\n",
        "    return a.values[random.randint(0,9)].capitalize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbQiEygYg-cI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "aec61687-2d79-43a6-83e8-66968dc0c5a4"
      },
      "source": [
        "get_recommendations('криминальное чтиво')"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Легенда о пианисте'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0PuXgGws1D2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_dialogs = pd.read_excel('dialog_talk_agent.en.ru.xlsx', names=['Context','Text Response'])\n",
        "df_dialogs.ffill(axis = 0,inplace=True)\n",
        "df_dialogs = df_dialogs.append({'Context' : 'Ты повторяешься' , 'Text Response' : 'Да и ты тоже'} , ignore_index=True)\n",
        "df_dialogs = df_dialogs.append({'Context' : 'Пойдем пить пиво' , 'Text Response' : 'Да возьми мне пару баночек'} , ignore_index=True)\n",
        "df_dialogs['lemmatized_text'] = df_dialogs['Context'].map(lambda x: tokenize_ru(str(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3fI2pUl1-RU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# stop = stopwords.words('english')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "\n",
        "x_tfidf=tfidf.fit_transform((df_dialogs['lemmatized_text'])).toarray() \n",
        "df_tfidf=pd.DataFrame(x_tfidf,columns=tfidf.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrrVHnAu2Iia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chat_tfidf(text):\n",
        "    lemma=tokenize_ru(text) # calling the function to perform text normalization\n",
        "    tf=tfidf.transform([lemma]).toarray() # applying tf-idf\n",
        "    cos=1-pairwise_distances(df_tfidf,tf,metric='cosine') # applying cosine similarity\n",
        "    index_value=cos.argmax() # getting index value \n",
        "    return df_dialogs['Text Response'].loc[index_value]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4_tvTJLNxEI",
        "colab_type": "code",
        "outputId": "7e6c7025-0a87-4bfd-8375-e29ea7720c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "chat_tfidf('Давай выпьем по пиву?')"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Да возьми мне пару баночек'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj5eETxcd9jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bot = telebot.TeleBot('1171053774:AAEKARUcqloBdKpZyT9_g4jnIKDcOUJYOKU')\n",
        "\n",
        "def random_movie():\n",
        "  return df_top250['original_title'][random.randint(0,len(df_top250['original_title'])-1)]\n",
        "  \n",
        "def get_button():\n",
        "  keyboard1 = telebot.types.ReplyKeyboardMarkup(True, True)\n",
        "  return keyboard1.row(random_movie().capitalize(),random_movie().capitalize(),random_movie().capitalize())\n",
        "  del keyboard1\n",
        "\n",
        "def get_elem(elem):\n",
        "  return elem[random.randint(0, len(elem)-1)]\n",
        "\n",
        "@bot.message_handler(commands=['start'])\n",
        "def start_message(message):\n",
        "  bot.send_sticker(message.chat.id, get_elem(stickers_ids) )\n",
        "  bot.send_message(message.chat.id, get_elem(questions) , parse_mode=\"markdown\" , reply_markup=get_button() ) \n",
        "\n",
        "@bot.message_handler(commands=['film'])\n",
        "def start_message(message):\n",
        "  bot.send_sticker(message.chat.id, get_elem(stickers_ids) )\n",
        "  bot.send_message(message.chat.id, get_elem(questions) , parse_mode=\"markdown\" )\n",
        "\n",
        "@bot.message_handler(content_types=['text'])\n",
        "def send_text(message):\n",
        "    #if message.text.lower().split(' ')[0] == 'rec':\n",
        "    if message.text.lower() in list(df_top250['original_title_lower']): # if message.text.lower() in list(df['original_title']):\n",
        "        #telebot.\n",
        "        #user = update.message.from_user\n",
        "        bot.send_message(message.chat.id,  str(get_recommendations(str(message.text.lower()  )))) #str(get_recommendations('The Hateful Eight')     )) #    str(get_recommendations('The Hateful Eight')) )\n",
        "        #bot.send_photo(message.chat.id,  'https://st.kp.yandex.net/images/film_iphone/iphone360_326.jpg' , caption = \"Карты\"  )\n",
        "        #print('You talk with user {} and his user ID: {} '.format(user['username'], user['id']))\n",
        "    elif message.text.lower().split(' ')[0] == 'r':\n",
        "        bot.send_photo(message.chat.id,  'https://st.kp.yandex.net/images/film_iphone/iphone360_326.jpg' , caption = \"Карты\"  )\n",
        "    elif message.text.lower().split(' ')[0] == 'w':\n",
        "        try:\n",
        "          content = str(wikipedia.summary(message.text.lower().split(' ')[1:]))\n",
        "          summary = summarize(content, split=True, word_count=50)\n",
        "        except BaseException as e:\n",
        "          summary = \"Попробуй еще\"\n",
        "        bot.send_message(message.chat.id, str(summary) )\n",
        "    elif message.text.lower().split(' ')[0] == 'wiki':\n",
        "        #content = str()\n",
        "        #summary = summarize(content, split=True, word_count=100)\n",
        "        try:\n",
        "          content = wikipedia.summary(message.text.split(' ')[1:])\n",
        "        except BaseException as e:\n",
        "          content = \"Попробуй еще\"        \n",
        "        bot.send_message(message.chat.id, content )\n",
        "    elif message.text.lower() == 'я тебя люблю':\n",
        "        bot.send_sticker(message.chat.id, 'AAMCAgADGQEAAyleiGTjHWbrAsBTwpvnfoF99txI1QACEgMAAmMr4gmTDo84BpQnPTDycA0ABAEAB20AA8ktAAIYBA')\n",
        "    else : \n",
        "        #bot.send_sticker(message.chat.id, 'CAADAgADZgkAAnlc4gmfCor5YbYYRAI')\n",
        "        time.sleep(1)\n",
        "        bot.send_message(message.chat.id, chat_tfidf(message.text.lower()) , reply_markup=get_button())\n",
        "        if bool(random.choices([True, False], weights=[40, 60])[0]):\n",
        "          bot.send_sticker(message.chat.id, get_elem(stickers_ids) )\n",
        "# get stiker id\n",
        "@bot.message_handler(content_types=['sticker'])\n",
        "def sticker_id(message):\n",
        "    print(message.sticker.file_id)\n",
        "\n",
        "\n",
        "bot.polling()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
